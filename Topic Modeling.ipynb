{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will go over some simple techniques in topic modeling: ```SVD```, ```NMF``` and ```LDA``` . We will also learn about the concepts of ```perplexity``` and ```topic coherence```.\n",
    "\n",
    "We will be using scikit-learn's ```20newsgroups``` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and survey the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the dataset\n",
    "# for our purposes it will suffice to only looking at the training set\n",
    "newsgroups = fetch_20newsgroups(subset = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of newsgroups categories\n",
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, let us only consider 7 categories of the newgroups. This will allow us to keep the corpora short and therefore our computing times low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism','comp.graphics','misc.forsale','rec.autos','sci.crypt','soc.religion.christian','talk.politics.guns']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups = fetch_20newsgroups(subset = 'train', categories = categories, remove = remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3983,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of documents in our dataset\n",
    "newsgroups.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Find an encyclopedia. Volume H. Now look up Hitler, Adolf. He had\n",
      "many more people than just Germans enamoured with him.\n",
      "\n",
      "P.\n",
      "\n",
      "=*=*=*=*=*=*=*=*=\n",
      "\n",
      "\n",
      "I don't know which passage you are refering to, but the passage I have\n",
      "often seen cited as an example of a mother image of God is Isaiah 49:15\n",
      "\"Can a woman forget her sucking child / that she should have no \n",
      "compassion / on the son of her womb? / Even these may forget, / \n",
      "yet I will not forget you.\" \n",
      " \n",
      "This passage is *not* a mother image of God at all. The mother here\n",
      "is the image of the best human constancy can show, and it is \n",
      "contrasted with the constancy of God. The mother figure here represents\n",
      "mankind, not God.\n",
      "-- \n",
      "==============================================================================\n",
      "Mark Baker                  | \"The task ... is not to cut down jungles, but \n",
      "aa888@Freenet.carleton.ca   | to irrigate deserts.\" -- C. S. Lewis\n",
      "==============================================================================\n",
      "\n",
      "[Luke 13:34   O Jerusalem, Jerusalem, killing the prophets and stoning those\n",
      "who are sent to you! How often would I have gathered your children together\n",
      "as a hen gathers her brood under her wings, and you would not!\n",
      "\n",
      "=*=*=*=*=*=*=*=*=\n",
      "\n",
      "Graeme> \tYes, that's known as \"Bresenhams Run Length Slice Algorithm for\n",
      "Graeme> Incremental lines\". See Fundamental Algorithms for Computer Graphics,\n",
      "Graeme> Springer-Verlag, Berlin Heidelberg 1985.\n",
      "\n",
      "\n",
      "Graeme> \tHmm. I don't think I can help you with this, but you might\n",
      "Graeme> take a look at the following:\n",
      "\n",
      "Graeme> \t\"Double-Step Incremental Generation of Lines and Circles\",\n",
      "Graeme> X. Wu and J. G. Rokne, Computer Graphics and Image processing,\n",
      "Graeme> Vol 37, No. 4, Mar. 1987, pp. 331-334\n",
      "\n",
      "Graeme> \t\"Double-Step Generation of Ellipses\", X. Wu and J. G. Rokne,\n",
      "Graeme> IEEE Computer Graphics & Applications, May 1989, pp. 56-69\n",
      "\n",
      "Another paper you might want to consider is:\n",
      "\n",
      "@article{fungdraw,\n",
      "  title=\"A Run-Length Slice Line Drawing Algorithm without Division Operations\",\n",
      "  author=\"Khun Yee Fung and Tina M. Nicholl and A. K. Dewdney\",\n",
      "  journal=\"Computer Graphics Forum\",\n",
      "  year=1992,\n",
      "  volume=11,\n",
      "  number=3,\n",
      "  pages=\"C-267--C-277\"\n",
      "}\n",
      "\n",
      "Khun Yee\n"
     ]
    }
   ],
   "source": [
    "# checkout the first three entries in the dataset\n",
    "# we will seperate each entry by '\\n\\n=*=*=*=*=*=*=*=*=\\n'\n",
    "print('\\n\\n=*=*=*=*=*=*=*=*=\\n'.join(newsgroups.data[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it is quite clear the the entries correspond to politics, religion and graphics respectively. Let's see if this is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain index of the target labels\n",
    "newsgroups.target[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'soc.religion.christian', 'comp.graphics'],\n",
       "      dtype='<U22')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain the corresponding elements from target_names\n",
    "np.array(newsgroups.target_names)[newsgroups.target[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first one has be categorized under atheism while we had guessed politics! This shows the intersectionality between atheism and politics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: 480 \n",
      "comp.graphics: 584 \n",
      "misc.forsale: 585 \n",
      "rec.autos: 594 \n",
      "sci.crypt: 595 \n",
      "soc.religion.christian: 599 \n",
      "talk.politics.guns: 546 \n"
     ]
    }
   ],
   "source": [
    "# number of documents in each category\n",
    "for ind in range(7):\n",
    "    target = newsgroups.target_names[ind]\n",
    "    print('{}: {} '.format(target,(newsgroups.target == ind).astype(int).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For puposes of topic modeling, we will first preprocess our data. We will therefore reduce all the words to lower case, remove stop words and apply stemming/lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem # This contains the PorterStemmer as well as the WordNetLemmatizer of nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the preprocessing function will stem all the words and convert them to lowercase\n",
    "preproc= lambda string: ' '.join(map(lambda word: porter.stem(word), string.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFind an encyclopedia. Volume H. Now look up Hitler, Adolf. He had\\nmany more people than just Germans enamoured with him.\\n\\nP.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'find an encyclopedia. volum h. now look up hitler, adolf. he had mani more peopl than just german enamour with him. p.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc(newsgroups.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of stop words before stemming: 179\n",
      "no. of stop words after stemming: 201\n"
     ]
    }
   ],
   "source": [
    "# after stemming the documents, some of the words may no longer be identified as stop-words as their stemmed version \n",
    "# will not be in the list of stop words. These words will therefore not be removed from the text\n",
    "# to cure this, apply stemming to the stop words and add them to the list of stop words\n",
    "# for e.g. see joeln's reply in this post: \n",
    "# https://stackoverflow.com/questions/57340142/user-warning-your-stop-words-may-be-inconsistent-with-your-preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('no. of stop words before stemming: {}'.format(len(stop_words)))\n",
    "stemmed_stop_words = set(map(lambda word: preproc(word), stop_words))\n",
    "stop_words.update(stemmed_stop_words)\n",
    "print('no. of stop words after stemming: {}'.format(len(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vectorizer with english stop words and the above preprocessing function\n",
    "# the vectorizer automatically ignores all punctuations\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, preprocessor = preproc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prarit/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the vectorizer to the data to obtain a bag-of-words matrix\n",
    "# To test the our preprocessing function, let's just try to fit the vectorizer only on the first document\n",
    "bow = vectorizer.fit_transform(newsgroups.data[0:1]).todense()\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adolf',\n",
       " 'enamour',\n",
       " 'encyclopedia',\n",
       " 'find',\n",
       " 'german',\n",
       " 'hitler',\n",
       " 'look',\n",
       " 'mani',\n",
       " 'peopl',\n",
       " 'volum']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the individual tokens can be obtained from the get_feature_names() method of the vectorizer\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature names produced by our vectorizer seem to match with the 1st document in our corpora. The preprocessing function that we created above seems to be working fine. Let's now fit the vectorizer to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of document-term matrix is: (3983, 38819)\n",
      "Sample of tokens generated: ['bells', 'belong', 'belonging', 'belongs', 'belorusskaya', 'belov', 'beloved', 'belt', 'belts', 'beltway']\n"
     ]
    }
   ],
   "source": [
    "bow = vectorizer.fit_transform(newsgroups.data).todense()\n",
    "print('shape of document-term matrix is: {}'.format(bow.shape))\n",
    "# print some of the tokens generated by the vectorizer\n",
    "# The first few ones seem to be numbers and emails etc. so I chose the indices to correspond to english words\n",
    "print('Sample of tokens generated: {}'.format(vectorizer.get_feature_names()[7500:7510]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38819,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD: Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now apply SVD for topic modeling. Also, we will use sklearn's [```randomized_svd``` ](https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_svd.html) since it is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "U, S, V = randomized_svd(bow, n_components = num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of U: (3983, 20)\n",
      "shape of S: (20,)\n",
      "shape of V: (20, 38819)\n"
     ]
    }
   ],
   "source": [
    "print('shape of U: {}'.format(U.shape))\n",
    "print('shape of S: {}'.format(S.shape))\n",
    "print('shape of V: {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([613.60405179, 574.17273772, 362.07868416, 344.71683751,\n",
       "       299.56507465, 255.67491512, 237.53532097, 212.53588514,\n",
       "       194.21425178, 180.92764131])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now display the top 10 words in each of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_k_word_indices = V.argsort(axis = 1)[:, :-top_k-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topic_words(topic_vector, vocab , top_k = 10):\n",
    "    top_word_indices = topic_vector.argsort()[:-top_k-1:-1]\n",
    "    words = vocab[top_word_indices]\n",
    "    weights = topic_vector[top_word_indices]\n",
    "    # zip the weights and words and make a string our of them\n",
    "    topic = '+'.join(map(lambda tup: str(tup[0])[:5]+'*'+tup[1] ,zip(weights,words)))\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 1\n",
      "0.873*db+0.217*mov+0.189*bh+0.142*cs+0.140*si+0.090*bit+0.083*byte+0.078*al+0.074*bl+0.074*file\n",
      "topic: 2\n",
      "0.378*file+0.239*use+0.204*imag+0.188*jpeg+0.154*edu+0.134*anonym+0.127*pub+0.115*ftp+0.112*mail+0.112*system\n",
      "topic: 3\n",
      "0.642*file+0.288*gun+0.139*congress+0.128*firearm+0.124*control+0.119*bill+0.110*state+0.102*mr+0.096*handgun+0.094*rkba\n",
      "topic: 4\n",
      "0.495*jpeg+0.339*imag+0.212*gif+0.167*color+0.139*format+0.101*version+0.085*display+0.084*convert+0.081*file+0.081*bit\n",
      "topic: 5\n",
      "0.208*edu+0.203*pub+0.136*mail+0.134*ftp+0.099*comput+0.097*data+0.089*internet+0.087*send+0.085*graphic+0.084*file\n",
      "topic: 6\n",
      "0.217*00+0.194*edu+0.163*god+0.110*data+0.109*graphic+0.099*128+0.098*3d+0.090*50+0.090*ray+0.085*40\n",
      "topic: 7\n",
      "0.660*00+0.245*50+0.211*appears+0.210*40+0.170*10+0.167*art+0.129*25+0.123*20+0.118*1st+0.112*80\n",
      "topic: 8\n",
      "0.267*anonym+0.171*post+0.149*edu+0.102*god+0.099*server+0.089*file+0.087*anon+0.083*jpeg+0.080*servic+0.073*mail\n",
      "topic: 9\n",
      "0.292*pub+0.232*eff+0.197*god+0.150*jpeg+0.142*electron+0.142*comput+0.112*ftp+0.107*atheist+0.104*privaci+0.102*org\n",
      "topic: 10\n",
      "0.268*god+0.231*imag+0.221*atheist+0.149*data+0.127*believ+0.117*internet+0.117*atheism+0.102*exist+0.099*email+0.094*religion\n"
     ]
    }
   ],
   "source": [
    "# let us view the top 10 topics\n",
    "for idx, vector in enumerate(V[:10]):\n",
    "    print('topic: {}'.format(idx+1))\n",
    "    print(show_topic_words(vector, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we see that topic 1, 2, 4, 5 and 8 seem to correspond to computer science and graphics related subjects. Similarly, topic 3 is clearly related to politics and guns. Topic 6 has a high component of 'god' but other words seem to be cs/graphics related. It is therefore much harder to tell what exactly does it represent. topic 7 mostly seems to be made up of number wich don't seem to tell us much either. topic 9 seems to an admixture of religion, atheism, politics and cs/graphics. Topic 10 can be clearly associated with atheism and religion. \n",
    "\n",
    "Thus we see that while some of the topics are very interpretable but still others are hard to make sense of. We also notice that our topics are largely dominated by words related to cs/graphics. I wonder why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if using a tf-idf vectorizer instead of count vectorizer makes a difference and gives a better selection of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stop_words, preprocessor = preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prarit/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3983, 38819)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat = vectorizer.fit_transform(newsgroups.data).todense()\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38819,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "U,S, V = randomized_svd(tfidf_mat, n_components = num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of U: (3983, 20)\n",
      "shape of S: (20,)\n",
      "shape of V: (20, 38819)\n"
     ]
    }
   ],
   "source": [
    "print('shape of U: {}'.format(U.shape))\n",
    "print('shape of S: {}'.format(S.shape))\n",
    "print('shape of V: {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.51841333, 4.92967855, 4.37497754, 3.83602063, 3.51430331,\n",
       "       3.3639679 , 3.04066331, 3.01200984, 2.9947416 , 2.94790133])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 1\n",
      "0.188*would+0.163*one+0.157*god+0.146*use+0.126*peopl+0.122*key+0.121*like+0.119*think+0.118*know+0.112*get\n",
      "topic: 2\n",
      "0.388*key+0.229*chip+0.206*encrypt+0.143*clipper+0.139*use+0.127*escrow+0.109*system+0.109*secur+0.104*bit+0.092*algorithm\n",
      "topic: 3\n",
      "0.330*key+0.199*god+0.184*chip+0.175*encrypt+0.124*clipper+0.116*escrow+0.088*secur+0.083*law+0.083*govern+0.076*believ\n",
      "topic: 4\n",
      "0.341*gun+0.178*car+0.116*right+0.101*peopl+0.099*crime+0.098*weapon+0.090*would+0.086*state+0.085*govern+0.085*law\n",
      "topic: 5\n",
      "0.337*00+0.289*car+0.178*god+0.143*key+0.130*price+0.115*sale+0.113*new+0.108*offer+0.104*drive+0.087*chip\n",
      "topic: 6\n",
      "0.352*00+0.238*gun+0.143*law+0.100*sale+0.084*control+0.083*offer+0.081*govern+0.081*weapon+0.078*crime+0.078*includ\n",
      "topic: 7\n",
      "0.240*00+0.200*could+0.180*ico+0.178*tek+0.176*bobbe+0.174*beauchain+0.174*sank+0.174*bronx+0.173*manhattan+0.168*stay\n",
      "topic: 8\n",
      "0.296*gun+0.253*god+0.174*key+0.146*file+0.146*bit+0.141*ico+0.139*tek+0.138*bobbe+0.137*beauchain+0.137*sank\n",
      "topic: 9\n",
      "0.476*00+0.283*key+0.159*bit+0.108*gun+0.100*know+0.097*think+0.090*file+0.077*christian+0.075*50+0.070*80\n",
      "topic: 10\n",
      "0.413*00+0.269*car+0.215*file+0.199*god+0.126*encrypt+0.125*law+0.100*govern+0.085*format+0.079*right+0.071*data\n"
     ]
    }
   ],
   "source": [
    "# let us view the top 10 topics\n",
    "for idx, vector in enumerate(V[:10]):\n",
    "    print('topic: {}'.format(idx+1))\n",
    "    print(show_topic_words(vector, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a vectorizer based on tfidf seems to a little better. Previously, using the simple vectorizer, we got a topic (topic 7 in the results from previous section) which was largely based on numbers and hence did not make much sense. But upon using tfidf, we do not get such a topic in our list of top 10 topics. Also the topics no longer seem to be dominated by cs/graphics related words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD + Tf-Idf + bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how things change if we also use bigrams in our vectorizer. \n",
    "\n",
    "Let us also, explicitly tell the vectorizer to ignore all number. We can do this by passing an appropriate regex as a token_patten to the vectorizer. The regex to use will be ```r'[a-z]{2,}'```. This will pick all words which are composed of two or more english alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'[a-z]{2,}'\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words, preprocessor = preproc, \n",
    "                             ngram_range = (1,2), token_pattern = regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prarit/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3983, 323956)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat = vectorizer.fit_transform(newsgroups.data).todense()\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323956,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "U, S, V = randomized_svd(tfidf_mat, n_components = num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of U: (3983, 20)\n",
      "shape of S: (20,)\n",
      "shape of V: (20, 323956)\n"
     ]
    }
   ],
   "source": [
    "print('shape of U: {}'.format(U.shape))\n",
    "print('shape of S: {}'.format(S.shape))\n",
    "print('shape of V: {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.42871277, 3.40051386, 2.96654507, 2.66881381, 2.60402419,\n",
       "       2.39154452, 2.31469002, 2.20558674, 2.1026558 , 2.04998648])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 1\n",
      "0.173*would+0.155*one+0.155*god+0.149*use+0.138*key+0.117*peopl+0.112*like+0.109*know+0.107*think+0.103*get\n",
      "topic: 2\n",
      "0.402*key+0.232*chip+0.228*encrypt+0.141*clipper+0.133*escrow+0.117*use+0.116*secur+0.114*bit+0.094*system+0.091*clipper chip\n",
      "topic: 3\n",
      "0.163*car+0.124*sale+0.118*offer+0.110*pleas+0.109*drive+0.106*price+0.101*mail+0.096*email+0.096*file+0.091*edu\n",
      "topic: 4\n",
      "0.179*ico+0.179*ico tek+0.179*vice ico+0.178*tek com+0.177*tek+0.176*bobbe vice+0.176*bobbe+0.175*blew bronx+0.175*com said+0.175*bob beauchain\n",
      "topic: 5\n",
      "0.325*gun+0.141*car+0.111*right+0.097*weapon+0.097*crime+0.094*law+0.090*state+0.088*govern+0.088*would+0.086*peopl\n",
      "topic: 6\n",
      "0.264*file+0.166*imag+0.121*program+0.112*format+0.110*ftp+0.095*gun+0.087*graphic+0.080*gif+0.076*data+0.072*code\n",
      "topic: 7\n",
      "0.165*law+0.129*offer+0.116*sale+0.114*govern+0.111*encrypt+0.109*god+0.098*technolog+0.094*new+0.087*gun+0.086*protect\n",
      "topic: 8\n",
      "0.341*car+0.129*encrypt+0.103*technolog+0.089*clipper+0.081*new+0.079*govern+0.077*privaci+0.068*administr+0.062*clipper chip+0.062*nist\n",
      "topic: 9\n",
      "0.148*group+0.144*post+0.100*mail+0.098*christian+0.097*pleas+0.094*would+0.089*book+0.078*church+0.074*newsgroup+0.074*email\n",
      "topic: 10\n",
      "0.187*car+0.159*key+0.137*edu+0.122*public+0.111*ripem+0.109*mail+0.106*god+0.100*gun+0.097*pgp+0.096*public key\n"
     ]
    }
   ],
   "source": [
    "# let us view the top 10 topics\n",
    "for idx, vector in enumerate(V[:10]):\n",
    "    print('topic: {}'.format(idx+1))\n",
    "    print(show_topic_words(vector, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that most of the topics are still composed of unigrams, though topic 4 seems to be composed of bigrams. However, just by reading it, it hard to tell what topic 4 corresponds to . Thus, at least for the current dataset, bigrams don't seem to help much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA: Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see how things change with [```Latent Dirichlet Allocation```](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) has been beautifully summarized/explained in [this](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) blog by Edwin Chen.\n",
    "\n",
    "Paraphrasing Edwin Chen:\n",
    "\n",
    "```LDA``` assumes that the documents in our corpora have been generated in the following manner:\n",
    "   \n",
    "1. The document d_i had a probability P(t_k | d_i) to corresponds to topic t_k\n",
    "\n",
    "2. The word w has probability p(w | t_k) to be emitted when the document's topic is t_k\n",
    " \n",
    "3. The r-th position in the document we sample a topic according to its probability p(t_k | d_i) and then sample a word w_r according to its probability to be emitted by the sampled topic i.e. p(w_r | t_k)\n",
    "\n",
    "Note that LDA's model of word generation is not a language model in that it does not take into account the previous sequence of words when generating the current word. \n",
    "\n",
    "Having understood the assumptions that LDA makes about the documents in our corpora, we can now use it to invert the process for topic modeling. The idea is as follows:\n",
    "\n",
    "1. Decide on the number of topics to model. Let's say this number is K\n",
    "\n",
    "2. To each word in the corpora, assign one of the K topics. Note that here multiple occurances of the same word are to be treated seperately. Thus the same word can and will be assigned different topics at different instances of its occurance.\n",
    "\n",
    "3. Step 2 inherently gives us an initial value for P(t_k | d_i) and p(w_r | t_k) for all the words, topics and documents :\n",
    "    \n",
    "    p(t_k | d_i) = $\\frac{\\text{no. of words in d_i with topic t_k}}{\\text{total number of words in d_i}}$\n",
    "    \n",
    "    p(w_r | t_k) = $\\frac{\\text{no. of times w_r occurs in the entire corpora with topic t_k}}{\\text{total number of words from the entire corpora in t_k }}$\n",
    "\n",
    "4. Having established an initial probability distribution, which random and hence bound ot be incorrect, we now recurssively correct it. The idea is to iterate over each word in each document and reassign a new topic to it in accordance with the current estimate of the probability distributions. Reiterating over this a large number of times is them expected to asymptote to the correct distibution. \n",
    "   \n",
    "   - The word w_r in a given document d_i is reassigned a topic t_k by sampling t_k with a probability: p(w_r | t_k) * p(t_k | d_i) . \n",
    "   \n",
    "   - Note that the probability distributions will change each time a word is assigned a new topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having understood the key ideas behind LDA, let us now apply it to our current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prarit/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3983, 32081)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = r'[a-z]{2,}'\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, preprocessor = preproc, token_pattern = regex)\n",
    "bow = vectorizer.fit_transform(newsgroups.data)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32081,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "LDA = LatentDirichletAllocation(n_components = num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3983, 20)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the topics for each document\n",
    "document_topic_mat = LDA.fit_transform(bow)\n",
    "document_topic_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 32081)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the words in each topic\n",
    "topics = LDA.components_\n",
    "topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 1\n",
      "194.0*jesu+126.2*one+107.8*jesus+87.36*matthew+77.30*christ+74.88*son+63.40*john+62.49*lord+60.35*say+59.55*name\n",
      "topic: 2\n",
      "33.99*use+32.08*get+29.86*san+23.47*like+21.80*newsgroup+21.11*media+19.59*post+17.82*ad+17.75*ca+15.28*mail\n",
      "topic: 3\n",
      "78.57*one+63.65*argument+58.26*true+58.04*fallaci+57.45*use+53.23*think+37.38*gener+35.76*premis+35.34*would+35.27*ad\n",
      "topic: 4\n",
      "232.1*church+105.1*cathol+104.2*pope+59.86*nist+47.41*gov+43.88*canon+41.29*ncsl+41.04*schism+40.74*bishop+33.44*would\n",
      "topic: 5\n",
      "268.8*jpeg+194.4*imag+189.7*color+165.2*gif+141.3*bit+120.5*file+99.33*format+94.39*use+86.38*card+78.41*st\n",
      "topic: 6\n",
      "55.52*univers+38.83*comput+35.66*confer+33.19*marriag+28.04*avail+27.89*call+26.05*room+25.73*research+25.25*crypto+25.11*program\n",
      "topic: 7\n",
      "746.1*gun+288.9*would+246.8*peopl+231.5*use+220.3*one+217.5*law+213.4*weapon+207.4*kill+204.2*crime+171.8*state\n",
      "topic: 8\n",
      "80.62*get+74.84*know+65.72*make+58.54*think+57.69*one+55.42*group+47.24*would+46.77*use+45.38*like+43.77*fbi\n",
      "topic: 9\n",
      "345.6*would+198.4*one+176.2*get+140.8*use+137.8*like+135.7*post+113.5*think+110.8*peopl+110.2*go+109.6*good\n",
      "topic: 10\n",
      "208.9*right+152.7*would+146.8*peopl+130.4*time+119.4*militia+111.6*year+109.2*get+100.8*one+100.5*could+99.64*well\n"
     ]
    }
   ],
   "source": [
    "# display the words in each topic\n",
    "for idx, vector in enumerate(topics[:10]):\n",
    "    print('topic: {}'.format(idx+1))\n",
    "    print(show_topic_words(vector, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics produced by LDA seem to be more coherent and interpretable :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4729.094631448444\n"
     ]
    }
   ],
   "source": [
    "perplexity_score = LDA.bound_\n",
    "print(perplexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
