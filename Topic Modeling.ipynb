{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will go over some simple techniques in topic modeling: ```SVD```, ```NMF``` and ```LDA``` . We will also learn about the concepts of ```perplexity``` and ```topic coherence```.\n",
    "\n",
    "We will be using scikit-learn's ```20newsgroups``` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and survey the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the dataset\n",
    "# for our purposes it will suffice to only looking at the training set\n",
    "newsgroups = fetch_20newsgroups(subset = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of newsgroups categories\n",
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, let us only consider 7 categories of the newgroups. This will allow us to keep the corpora short and therefore our computing times low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism','comp.graphics','misc.forsale','rec.autos','sci.crypt','soc.religion.christian','talk.politics.guns']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups = fetch_20newsgroups(subset = 'train', categories = categories, remove = remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3983,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of documents in our dataset\n",
    "newsgroups.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Find an encyclopedia. Volume H. Now look up Hitler, Adolf. He had\n",
      "many more people than just Germans enamoured with him.\n",
      "\n",
      "P.\n",
      "\n",
      "=*=*=*=*=*=*=*=*=\n",
      "\n",
      "\n",
      "I don't know which passage you are refering to, but the passage I have\n",
      "often seen cited as an example of a mother image of God is Isaiah 49:15\n",
      "\"Can a woman forget her sucking child / that she should have no \n",
      "compassion / on the son of her womb? / Even these may forget, / \n",
      "yet I will not forget you.\" \n",
      " \n",
      "This passage is *not* a mother image of God at all. The mother here\n",
      "is the image of the best human constancy can show, and it is \n",
      "contrasted with the constancy of God. The mother figure here represents\n",
      "mankind, not God.\n",
      "-- \n",
      "==============================================================================\n",
      "Mark Baker                  | \"The task ... is not to cut down jungles, but \n",
      "aa888@Freenet.carleton.ca   | to irrigate deserts.\" -- C. S. Lewis\n",
      "==============================================================================\n",
      "\n",
      "[Luke 13:34   O Jerusalem, Jerusalem, killing the prophets and stoning those\n",
      "who are sent to you! How often would I have gathered your children together\n",
      "as a hen gathers her brood under her wings, and you would not!\n",
      "\n",
      "=*=*=*=*=*=*=*=*=\n",
      "\n",
      "Graeme> \tYes, that's known as \"Bresenhams Run Length Slice Algorithm for\n",
      "Graeme> Incremental lines\". See Fundamental Algorithms for Computer Graphics,\n",
      "Graeme> Springer-Verlag, Berlin Heidelberg 1985.\n",
      "\n",
      "\n",
      "Graeme> \tHmm. I don't think I can help you with this, but you might\n",
      "Graeme> take a look at the following:\n",
      "\n",
      "Graeme> \t\"Double-Step Incremental Generation of Lines and Circles\",\n",
      "Graeme> X. Wu and J. G. Rokne, Computer Graphics and Image processing,\n",
      "Graeme> Vol 37, No. 4, Mar. 1987, pp. 331-334\n",
      "\n",
      "Graeme> \t\"Double-Step Generation of Ellipses\", X. Wu and J. G. Rokne,\n",
      "Graeme> IEEE Computer Graphics & Applications, May 1989, pp. 56-69\n",
      "\n",
      "Another paper you might want to consider is:\n",
      "\n",
      "@article{fungdraw,\n",
      "  title=\"A Run-Length Slice Line Drawing Algorithm without Division Operations\",\n",
      "  author=\"Khun Yee Fung and Tina M. Nicholl and A. K. Dewdney\",\n",
      "  journal=\"Computer Graphics Forum\",\n",
      "  year=1992,\n",
      "  volume=11,\n",
      "  number=3,\n",
      "  pages=\"C-267--C-277\"\n",
      "}\n",
      "\n",
      "Khun Yee\n"
     ]
    }
   ],
   "source": [
    "# checkout the first three entries in the dataset\n",
    "# we will seperate each entry by '\\n\\n=*=*=*=*=*=*=*=*=\\n'\n",
    "print('\\n\\n=*=*=*=*=*=*=*=*=\\n'.join(newsgroups.data[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it is quite clear the the entries correspond to politics, religion and graphics respectively. Let's see if this is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain index of the target labels\n",
    "newsgroups.target[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'soc.religion.christian', 'comp.graphics'],\n",
       "      dtype='<U22')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain the corresponding elements from target_names\n",
    "np.array(newsgroups.target_names)[newsgroups.target[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first one has be categorized under atheism while we had guessed politics! This shows the intersectionality between atheism and politics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: 480 \n",
      "comp.graphics: 584 \n",
      "misc.forsale: 585 \n",
      "rec.autos: 594 \n",
      "sci.crypt: 595 \n",
      "soc.religion.christian: 599 \n",
      "talk.politics.guns: 546 \n"
     ]
    }
   ],
   "source": [
    "# number of documents in each category\n",
    "for ind in range(7):\n",
    "    target = newsgroups.target_names[ind]\n",
    "    print('{}: {} '.format(target,(newsgroups.target == ind).astype(int).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For puposes of topic modeling, we will first preprocess our data. We will therefore reduce all the words to lower case, remove stop words and apply stemming/lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem # This contains the PorterStemmer as well as the WordNetLemmatizer of nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the preprocessing function will stem all the words and convert them to lowercase\n",
    "preproc= lambda string: ' '.join(map(lambda word: porter.stem(word), string.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFind an encyclopedia. Volume H. Now look up Hitler, Adolf. He had\\nmany more people than just Germans enamoured with him.\\n\\nP.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'find an encyclopedia. volum h. now look up hitler, adolf. he had mani more peopl than just german enamour with him. p.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc(newsgroups.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of stop words before stemming: 179\n",
      "no. of stop words after stemming: 201\n"
     ]
    }
   ],
   "source": [
    "# after stemming the documents, some of the words may no longer be identified as stop-words as their stemmed version \n",
    "# will not be in the list of stop words. These words will therefore not be removed from the text\n",
    "# to cure this, apply stemming to the stop words and add them to the list of stop words\n",
    "# for e.g. see joeln's reply in this post: \n",
    "# https://stackoverflow.com/questions/57340142/user-warning-your-stop-words-may-be-inconsistent-with-your-preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('no. of stop words before stemming: {}'.format(len(stop_words)))\n",
    "stemmed_stop_words = set(map(lambda word: preproc(word), stop_words))\n",
    "stop_words.update(stemmed_stop_words)\n",
    "print('no. of stop words after stemming: {}'.format(len(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vectorizer with english stop words and the above preprocessing function\n",
    "# the vectorizer automatically ignores all punctuations\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, preprocessor = preproc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the vectorizer to the data to obtain a bag-of-words matrix\n",
    "# To test the our preprocessing function, let's just try to fit the vectorizer only on the first document\n",
    "bow = vectorizer.fit_transform(newsgroups.data[0:1]).todense()\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adolf',\n",
       " 'enamour',\n",
       " 'encyclopedia',\n",
       " 'find',\n",
       " 'german',\n",
       " 'hitler',\n",
       " 'look',\n",
       " 'mani',\n",
       " 'peopl',\n",
       " 'volum']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the individual tokens can be obtained from the get_feature_names() attribute of the vectorizer\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature names produced by our vectorizer seem to match with the 1st document in our corpora. The preprocessing function that we created above seems to be working fine. Let's now fit the vectorizer to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of document-term matrix is: (3983, 38819)\n",
      "Sample of tokens generated: ['bells', 'belong', 'belonging', 'belongs', 'belorusskaya', 'belov', 'beloved', 'belt', 'belts', 'beltway']\n"
     ]
    }
   ],
   "source": [
    "bow = vectorizer.fit_transform(newsgroups.data).todense()\n",
    "print('shape of document-term matrix is: {}'.format(bow.shape))\n",
    "# print some of the tokens generated by the vectorizer\n",
    "# The first few ones seem to be numbers and emails etc. so I chose the indices to correspond to english words\n",
    "print('Sample of tokens generated: {}'.format(vectorizer.get_feature_names()[7500:7510]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38819,)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD: Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now apply SVD for topic modeling. Also, we will use sklearn's [```randomized_svd``` ](https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_svd.html) since it is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "U, S, V = randomized_svd(bow, n_components = num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of U: (3983, 20)\n",
      "shape of S: (20,)\n",
      "shape of V: (20, 38819)\n"
     ]
    }
   ],
   "source": [
    "print('shape of U: {}'.format(U.shape))\n",
    "print('shape of S: {}'.format(S.shape))\n",
    "print('shape of V: {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([613.60405179, 574.17273772, 362.07868416, 344.71683751,\n",
       "       299.56507465, 255.67491512, 237.53532097, 212.53588514,\n",
       "       194.21425178, 180.92764131])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now display the top 10 words in each of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_k_word_indices = V.argsort(axis = 1)[:, :-top_k-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topic_words(topic_vector, top_k = 10):\n",
    "    top_word_indices = topic_vector.argsort()[:-top_k-1:-1]\n",
    "    words = vocab[top_word_indices]\n",
    "    weights = topic_vector[top_word_indices]\n",
    "    # zip the weights and words and make a string our of them\n",
    "    topic = '+'.join(map(lambda tup: str(tup[0])[:5]+'*'+tup[1] ,zip(weights,words)))\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 1\n",
      "0.873*db+0.217*mov+0.189*bh+0.142*cs+0.140*si+0.090*bit+0.083*byte+0.078*al+0.074*bl+0.074*file\n",
      "topic: 2\n",
      "0.378*file+0.239*use+0.204*imag+0.188*jpeg+0.154*edu+0.134*anonym+0.127*pub+0.115*ftp+0.112*mail+0.112*system\n",
      "topic: 3\n",
      "0.642*file+0.288*gun+0.139*congress+0.128*firearm+0.124*control+0.119*bill+0.110*state+0.102*mr+0.096*handgun+0.094*rkba\n",
      "topic: 4\n",
      "0.495*jpeg+0.339*imag+0.212*gif+0.167*color+0.139*format+0.101*version+0.085*display+0.084*convert+0.081*file+0.081*bit\n",
      "topic: 5\n",
      "0.208*edu+0.203*pub+0.136*mail+0.134*ftp+0.099*comput+0.097*data+0.089*internet+0.087*send+0.085*graphic+0.084*file\n",
      "topic: 6\n",
      "0.217*00+0.194*edu+0.163*god+0.110*data+0.109*graphic+0.099*128+0.098*3d+0.090*50+0.090*ray+0.085*40\n",
      "topic: 7\n",
      "0.660*00+0.245*50+0.211*appears+0.210*40+0.170*10+0.167*art+0.129*25+0.123*20+0.118*1st+0.112*80\n",
      "topic: 8\n",
      "0.267*anonym+0.171*post+0.149*edu+0.102*god+0.099*server+0.089*file+0.087*anon+0.083*jpeg+0.080*servic+0.073*mail\n",
      "topic: 9\n",
      "0.292*pub+0.232*eff+0.197*god+0.150*jpeg+0.142*electron+0.142*comput+0.112*ftp+0.107*atheist+0.104*privaci+0.102*org\n",
      "topic: 10\n",
      "0.268*god+0.231*imag+0.221*atheist+0.149*data+0.127*believ+0.117*internet+0.117*atheism+0.102*exist+0.099*email+0.094*religion\n"
     ]
    }
   ],
   "source": [
    "# let us view the top 10 topics\n",
    "for idx, vector in enumerate(V[:10]):\n",
    "    print('topic: {}'.format(idx+1))\n",
    "    print(show_topic_words(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we see that topic 1, 2, 4, 5 and 8 seem to correspond to computer science and graphics related subjects. Similarly, topic 3 is clearly related to politics and guns. Topic 6 has a high component of 'god' but other words seem to be cs/graphics related. It is therefore much harder to tell what exactly does it represent. topic 7 mostly seems to be made up of number wich don't seem to tell us much either. topic 9 seems to an admixture of religion, atheism, politics and cs/graphics. Topic 10 can be clearly associated with atheism and religion. \n",
    "\n",
    "Thus we see that while some of the topics are very interpretable but still others are hard to make sense of. We also notice that our topics are largely dominated by words related to cs/graphics. I wonder why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if using a tf-idf vectorizer instead of count vectorizer makes a difference and gives a better selection of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stop_words, preprocessor = preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prarit/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3983, 38819)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat = vectorizer.fit_transform(newsgroups.data).todense()\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "U,S, V = randomized_svd(tfidf_mat, n_components = num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of U: (3983, 20)\n",
      "shape of S: (20,)\n",
      "shape of V: (20, 38819)\n"
     ]
    }
   ],
   "source": [
    "print('shape of U: {}'.format(U.shape))\n",
    "print('shape of S: {}'.format(S.shape))\n",
    "print('shape of V: {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.51841333, 4.92967855, 4.37497754, 3.83602063, 3.51430331,\n",
       "       3.3639679 , 3.04066331, 3.01200984, 2.9947416 , 2.94790133])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 1\n",
      "0.188*would+0.163*one+0.157*god+0.146*use+0.126*peopl+0.122*key+0.121*like+0.119*think+0.118*know+0.112*get\n",
      "topic: 2\n",
      "0.388*key+0.229*chip+0.206*encrypt+0.143*clipper+0.139*use+0.127*escrow+0.109*system+0.109*secur+0.104*bit+0.092*algorithm\n",
      "topic: 3\n",
      "0.330*key+0.199*god+0.184*chip+0.175*encrypt+0.124*clipper+0.116*escrow+0.088*secur+0.083*law+0.083*govern+0.076*believ\n",
      "topic: 4\n",
      "0.341*gun+0.178*car+0.116*right+0.101*peopl+0.099*crime+0.098*weapon+0.090*would+0.086*state+0.085*govern+0.085*law\n",
      "topic: 5\n",
      "0.337*00+0.289*car+0.178*god+0.143*key+0.130*price+0.115*sale+0.113*new+0.108*offer+0.104*drive+0.087*chip\n",
      "topic: 6\n",
      "0.352*00+0.238*gun+0.143*law+0.100*sale+0.084*control+0.083*offer+0.081*govern+0.081*weapon+0.078*crime+0.078*includ\n",
      "topic: 7\n",
      "0.240*00+0.200*could+0.180*ico+0.178*tek+0.176*bobbe+0.174*beauchain+0.174*sank+0.174*bronx+0.173*manhattan+0.168*stay\n",
      "topic: 8\n",
      "0.296*gun+0.253*god+0.174*key+0.146*file+0.146*bit+0.141*ico+0.139*tek+0.138*bobbe+0.137*beauchain+0.137*sank\n",
      "topic: 9\n",
      "0.476*00+0.283*key+0.159*bit+0.108*gun+0.100*know+0.097*think+0.090*file+0.077*christian+0.075*50+0.070*80\n",
      "topic: 10\n",
      "0.413*00+0.269*car+0.215*file+0.199*god+0.126*encrypt+0.125*law+0.100*govern+0.085*format+0.079*right+0.071*data\n"
     ]
    }
   ],
   "source": [
    "# let us view the top 10 topics\n",
    "for idx, vector in enumerate(V[:10]):\n",
    "    print('topic: {}'.format(idx+1))\n",
    "    print(show_topic_words(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a vectorizer based on tfidf seems to a little better. Previously, using the simple vectorizer, we got a topic (topic 7 in the results from previous section) which was largely based on numbers and hence did not make much sense. But upon using tfidf, we do not get such a topic in our list of top 10 topics. Also the topics no longer seem to be dominated by cs/graphics related words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
